from langchain_ollama import OllamaLLM as Ollama
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langdetect import detect
from duckduckgo_search import DDGS
import os

# === Memory Configuration ===
conversation_memory = []
MAX_MEMORY = 3  # last 3 user-bot turns

MARINE_KEYWORDS = [
    "ocean", "marine", "fish", "coral", "reef", "plankton", "ecosystem", "biodiversity",
    "sea", "pollution", "climate", "acidification", "species", "coast", "seagrass",
    "plastic", "conservation", "mammals", "jellyfish", "tuna", "microplastics", "algae"
]

# === Gentle Input Categories ===
GREETINGS = [
    "hello", "hi", "hey", "bonjour", "salut", "hola", "yo", "heya", "hi there",
    "good morning", "good afternoon", "good evening"
]

THANKS = [
    "thanks", "thank you", "thank u", "ty", "merci", "thanks a lot", "many thanks",
    "thx", "much appreciated", "gracias", "danke", "by", "bye"
]

NEUTRAL_POLITE = [
    "ok", "okay", "good", "great", "awesome", "nice", "cool", "yes", "no",
    "alright", "yep", "yup", "sure", "fine", "bien", "super", "parfait", "d'accord", "c'est bon"
]

# === Load LLM only once globally ===
llm = Ollama(
    model="mistral",
    temperature=0.7,
    top_p=0.9,
    top_k=50
)
def is_marine_related(query):
    return any(keyword in query.lower() for keyword in MARINE_KEYWORDS)
# === Load Vectorstore ONLY when needed ===
def load_retriever():
    embeddings = OllamaEmbeddings(model="mistral")
    base_dir = os.path.dirname(os.path.abspath(__file__))
    path = os.path.join(base_dir, "vectorstore")
    vectorstore = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
    return vectorstore.as_retriever(search_kwargs={"k": 3})

# === DuckDuckGo Fallback Search ===
def fallback_duckduckgo(query, lang="en"):
    with DDGS() as ddgs:
        results = list(ddgs.text(query, region='fr-fr' if lang == 'fr' else 'wt-wt', safesearch='moderate', max_results=3))
        return "\n".join([r['body'] for r in results if 'body' in r])

# === Main Response Function ===
def get_response(user_query):
    global conversation_memory
    query = user_query.lower().strip()

    # Load retriever at each query (safe for Django)
    retriever = load_retriever()

    # Empty input handler
    if not query:
        return "ðŸ’¬ Please ask a question about marine ecosystems so I can help you (EN/FR)."

    if query in GREETINGS:
        lang = "fr" if query in ["bonjour", "salut"] else "en"
        return "ðŸ‘‹ Bonjour ! Comment puis-je vous aider Ã  propos des Ã©cosystÃ¨mes marins ?" if lang == "fr" else "ðŸ‘‹ Hello! How can I help you with marine ecosystems today?"

    elif query in THANKS:
        lang = "fr" if "merci" in query else "en"
        return "ðŸ˜Š Avec plaisir ! Si vous avez d'autres questions, je suis lÃ  pour vous aider." if lang == "fr" else "ðŸ˜Š You're very welcome! I'm always here to help you."

    elif query in NEUTRAL_POLITE:
        lang = detect(query)
        return "âœ¨ Câ€™est notÃ© ! Si vous avez une question sur les Ã©cosystÃ¨mes marins, nâ€™hÃ©sitez pas." if lang == "fr" else "âœ¨ Got it! Let me know if youâ€™d like to know more about marine life."

    # Language detection
    lang = detect(user_query)
    if lang not in ["en", "fr"]:
        return "âŒ Sorry, I can only understand and respond in English or French. Please rephrase your question." \
            if lang == "en" else "âŒ DÃ©solÃ©, je ne peux comprendre que l'anglais et le franÃ§ais. Veuillez reformuler votre question."

    # Retrieve knowledge
    context = retriever.invoke(user_query)
    context_text = "\n".join([doc.page_content for doc in context])
    fallback_used = False

    if not context_text.strip() and is_marine_related(query):
        fallback_used = True
        context_text = fallback_duckduckgo(user_query, lang)

    online_note = {
        "en": "ðŸ”Ž I looked online to complete my answer as this wasnâ€™t in my knowledge base.",
        "fr": "ðŸ”Ž J'ai complÃ©tÃ© ma rÃ©ponse grÃ¢ce Ã  des sources en ligne, car cela n'Ã©tait pas prÃ©sent dans ma base de connaissances."
    }[lang] if fallback_used else ""

    # Manage memory
    conversation_memory.append(f"User: {user_query}")
    if len(conversation_memory) > MAX_MEMORY * 2:
        conversation_memory = conversation_memory[-MAX_MEMORY * 2:]
    memory_text = "\n".join(conversation_memory)

    # === Prompt Construction ===
    if lang == "en":
        prompt = f"""
You are "Maren", a highly trained marine ecosystems expert AI. You communicate clearly, concisely, and kindly like a friendly biologist would. Stay focused solely on marine-related questions: oceans, coral reefs, marine species, pollution, biodiversity, climate impacts, ecosystems, etc.

Refuse off-topic questions gently and encourage rephrasing to stay within marine themes.

Your behavior guide:
- If the user greets you, respond warmly but briefly.
- If the user thanks you, acknowledge it with warmth.
- If information was retrieved via online search, mention that transparently but humbly.
- Use retrieved documents and recent conversation history to ground your answer.
- Never invent facts. If uncertain, say so clearly and helpfully.
- Maximum response length: **60 words**
- Answer in the same language as the userâ€™s input (English or French).

{online_note}

---
ðŸ§  Conversation Memory:
{memory_text}
---
ðŸ“š Retrieved Knowledge:
{context_text}
---
â“User Question:
{user_query}
---
Your Response:
"""
    else:
        prompt = f"""
Vous Ãªtes "Maren", une intelligence artificielle experte en Ã©cosystÃ¨mes marins. Vous communiquez avec clartÃ©, concision et bienveillance, comme un biologiste marin amical. RÃ©pondez uniquement aux questions liÃ©es Ã  lâ€™ocÃ©an, aux rÃ©cifs coralliens, Ã  la biodiversitÃ© marine, Ã  la pollution, au climat et aux Ã©cosystÃ¨mes.

Refusez poliment les questions hors sujet en invitant lâ€™utilisateur Ã  revenir Ã  des sujets marins.

Guide comportemental :
- Si l'utilisateur vous salue, rÃ©pondez briÃ¨vement et chaleureusement.
- Sâ€™il vous remercie, rÃ©pondez avec bienveillance.
- Si lâ€™information provient dâ€™une recherche en ligne, mentionnez-le avec transparence.
- Appuyez vos rÃ©ponses sur les documents retrouvÃ©s et la mÃ©moire de conversation.
- Ne jamais inventer de faits. Si vous nâ€™Ãªtes pas sÃ»r, dites-le.
- Longueur maximale : **60 mots**
- RÃ©pondez dans la langue de la question.

{online_note}

---
ðŸ§  MÃ©moire de conversation :
{memory_text}
---
ðŸ“š Connaissances retrouvÃ©es :
{context_text}
---
â“Question utilisateur :
{user_query}
---
Votre rÃ©ponse :
"""

  
    # Get LLM response
    response = llm.invoke(prompt).strip()

    # Update memory
    conversation_memory.append(f"Bot: {response}")
    if len(conversation_memory) > MAX_MEMORY * 2:
        conversation_memory = conversation_memory[-MAX_MEMORY * 2:]

    return response